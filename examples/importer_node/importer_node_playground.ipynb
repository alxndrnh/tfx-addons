{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.2 (default, Jan 31 2023, 18:34:03) \\n[GCC 12.2.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Python Version\n",
    "import shutil\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade pip\n",
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check TF & TFX Versioning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tfx import v1 as tfx\n",
    "print(tfx.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Variables as importer_node_playground\n",
    "import os\n",
    "\n",
    "# Pipeline name\n",
    "PIPELINE_NAME = \"importer_node_playground\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = './artifacts'\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "# Folder path to data\n",
    "DATA_ROOT = './data/'\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer_variable = tfx.dsl.Importer\n",
    "\n",
    "external_examples = tfx.types.standard_artifacts.Examples\n",
    "\n",
    "external_examples.PROPERTIES['span'] = 1\n",
    "external_examples.PROPERTIES['version'] = 1\n",
    "external_examples.PROPERTIES['split_names'] = \"Split_train\"\n",
    "\n",
    "# external_examples = './artifacts/CopyExampleGen/examples/'\n",
    "\n",
    "print(external_examples.PROPERTIES['span'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "### [USE CASE 1]\n",
    "### Tfrecords are already created\n",
    "\n",
    "### This component will:\n",
    "### 1. Accept a dict with with {'split_name': './path/to/split_name/tfrecords.gz'}\n",
    "### 2. Add them to a folder to follow Example Artifact directory structure\n",
    "### 3. Import them with Importer node to register the external resource into MLMD\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "# Create pipeline to run Importer node\n",
    "def _create_pipeline(\n",
    "  pipeline_name: str,\n",
    "  pipeline_root: str,\n",
    "  data_root: str,\n",
    "  metadata_path: str\n",
    "  ) -> tfx.dsl.Pipeline:\n",
    "\n",
    "\n",
    "  ### Get external source data\n",
    "  # Source directory of external Examples Artifacts\n",
    "  source_examples_artifact_uri = '../artifacts/'\n",
    "\n",
    "  # Destination directory for source\n",
    "  destination_examples_artifact_uri = './artifacts/'\n",
    "\n",
    "  # Get all files from source_examples_artifact_uri\n",
    "  files = os.listdir(source_examples_artifact_uri)\n",
    "\n",
    "  # Import source files to destination\n",
    "  shutil.copytree(source_examples_artifact_uri, destination_examples_artifact_uri, dirs_exist_ok=True)\n",
    "\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Importer Node\n",
    "  # examples_importer = tfx.dsl.Importer(\n",
    "  #     source_uri='./tfrecords/1/',\n",
    "  #     artifact_type=tfx.types.standard_artifacts.Examples).with_id(\n",
    "  #         'examples_importer')\n",
    "\n",
    "  # print(\"IMPORTER_NODE: \", importer_node.outputs)\n",
    "  \n",
    "\n",
    "  # EXAMPLEGEN COMPONENT OUTPUT\n",
    "  # Brings data into the pipeline.\n",
    "  # example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "  # print(\"EXAMPLE_GEN: \", example_gen.outputs['examples'])\n",
    "\n",
    "  # print(\"tfx.components.CsvExampleGen.outputs['examples'] output: \\n\\n\",\n",
    "  #   \"Artifact Type: \", example_gen.outputs['examples']._artifact_type, \"\\n\"\n",
    "  #   \"Producer Component Id: \", example_gen.outputs['examples'].producer_component_id, \"\\n\"\n",
    "  #   \"Output Key: \", example_gen.outputs['examples'].output_key, \"\\n\"\n",
    "  #   \"Additional Properties: \", example_gen.outputs['examples'].additional_properties, \"\\n\"\n",
    "  #   \"Additional Custom Properties: \", example_gen.outputs['examples'].additional_custom_properties, \"\\n\"\n",
    "  # )\n",
    "\n",
    "\n",
    "  # Computes statistics over data for visualization and schema generation.\n",
    "  # statistics_gen = tfx.components.StatisticsGen(\n",
    "  #     examples=importer_node.outputs['examples'])\n",
    "  \n",
    "\n",
    "  components = [\n",
    "    example_gen\n",
    "    # examples_importer\n",
    "    # statistics_gen\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "    components=components\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      metadata_path=METADATA_PATH)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "### [USE CASE 2]\n",
    "### Raw data only\n",
    "\n",
    "### This component will:\n",
    "### 1. Accept a dict with with {'split_name': './path/to/rawdata}\n",
    "### 2. Convert these files to tfrecords\n",
    "###       - Register under MLMD using Importer(?)\n",
    "### 3. Format tfrecords to Examples Artifact directory structure\n",
    "###       - Register under MLMD using Importer(?)\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "# Create pipeline to run Importer node\n",
    "def _create_pipeline(\n",
    "  pipeline_name: str,\n",
    "  pipeline_root: str,\n",
    "  data_root: str,\n",
    "  metadata_path: str\n",
    "  ) -> tfx.dsl.Pipeline:\n",
    "\n",
    "  ######################################################################\n",
    "\n",
    "  # 1. Accept a dict with with {'split_name': './path/to/rawdata}\n",
    "  # Source directory of external Examples Artifacts\n",
    "  source_examples_artifact_uri = {\n",
    "    'split_train':'../raw_data/split_train.csv',\n",
    "    'split_eval': '../raw_data/'\n",
    "  }\n",
    "\n",
    "  ######################################################################\n",
    "\n",
    "  # 2. Convert these files to tfrecords w/o using Beam executor\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Brings data into the pipeline.\n",
    "  # example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # print(\"EXAMPLE_GEN: \", example_gen.outputs['examples'])\n",
    "\n",
    "  # print(\"tfx.components.CsvExampleGen.outputs['examples'] output: \\n\\n\",\n",
    "  #   \"Artifact Type: \", example_gen.outputs['examples']._artifact_type, \"\\n\"\n",
    "  #   \"Producer Component Id: \", example_gen.outputs['examples'].producer_component_id, \"\\n\"\n",
    "  #   \"Output Key: \", example_gen.outputs['examples'].output_key, \"\\n\"\n",
    "  #   \"Additional Properties: \", example_gen.outputs['examples'].additional_properties, \"\\n\"\n",
    "  #   \"Additional Custom Properties: \", example_gen.outputs['examples'].additional_custom_properties, \"\\n\"\n",
    "  # )\n",
    "\n",
    "  # Importer Node\n",
    "  importer_node = tfx.dsl.Importer(\n",
    "    source_uri='./artifacts/CopyExampleGen/examples/1/Split-eval/',\n",
    "    artifact_type=tfx.types.standard_artifacts.Examples,\n",
    "    output_key='examples'\n",
    "    ).with_id('CopyExampleGen')\n",
    "\n",
    "  # example_gen = tfx.components.ImportExampleGen(input_base='./artifacts/CopyExampleGen/examples/1/Split-eval/')\n",
    "\n",
    "  print(\"IMPORTER_NODE: \", importer_node.outputs['examples'])\n",
    "  \n",
    "  # Computes statistics over data for visualization and schema generation.\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=importer_node.outputs['examples'])\n",
    "  \n",
    "\n",
    "  components = [\n",
    "    # example_gen,\n",
    "    importer_node,\n",
    "    statistics_gen\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "    components=components\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SPLIT_TRAIN] View Dataset Artifact as tf.record\n",
    "train_uri = os.path.join('./artifacts/CsvExampleGen/examples/1/', 'Split-train')\n",
    "\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "for tfrecord in dataset:\n",
    "  # Prints out tf.record\n",
    "  print(tfrecord)\n",
    "  \n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "\n",
    "  # Prints out parsed tfrecord as JSON\n",
    "  example.ParseFromString(serialized_example)\n",
    "  print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SPLIT_EVAL] View Dataset Artifact\n",
    "eval_uri = os.path.join('./artifacts/CsvExampleGen/examples/1/', 'Split-eval')\n",
    "\n",
    "tfrecord_filenames = [os.path.join(eval_uri, name)\n",
    "                      for name in os.listdir(eval_uri)]\n",
    "\n",
    "print(tfrecord_filenames)\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "for tfrecord in dataset.take(3):\n",
    "  # Prints out tf.record\n",
    "  print(tfrecord)\n",
    "\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "  \n",
    "  # Prints out parsed tfrecord as JSON\n",
    "  example.ParseFromString(serialized_example)\n",
    "  print(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx-3.8.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a21382bcf4cd2ec5bec501ec961be81f5e055eb8968eab00b0db6b308c7e2937"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
