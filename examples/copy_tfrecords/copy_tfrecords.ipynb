{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/google/home/alexanderho/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages (23.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 22:20:30.377452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-06 22:20:30.550457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-06 22:20:30.550507: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-06 22:20:31.852449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 22:20:31.852599: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 22:20:31.852614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "#Check Python Version\n",
    "import shutil\n",
    "import sys\n",
    "sys.version\n",
    "\n",
    "\n",
    "#Upgrade pip\n",
    "%pip install --upgrade pip\n",
    "\n",
    "\n",
    "#Check TF & TFX Versioning\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tfx import v1 as tfx\n",
    "print(tfx.__version__)\n",
    "\n",
    "\n",
    "#Setup Variables as examplegen_playground\n",
    "import os\n",
    "\n",
    "# Pipeline name\n",
    "PIPELINE_NAME = \"copy_tfrecords\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = './artifacts'\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "# Folder path to data\n",
    "DATA_ROOT = './data/'\n",
    "\n",
    "# Folder path to tfrecords\n",
    "TFRECORDS_TRAIN_DATA_PATH = '../../../../Documents/tfrecord1_split_train/data_tfrecord-00000-of-00001.gz'\n",
    "TFRECORDS_EVAL_DATA_PATH = '../../../../Documents/tfrecord2_split_eval/data_tfrecord-00000-of-00001.gz'\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "from tfx.v1.types.standard_artifacts import Examples\n",
    "from typing import Any, Dict, List, Optional, Type, Union\n",
    "\n",
    "from hello import component\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  # User input dictionary with split-names and their resepctive uri to tfrecords\n",
    "  tfrecords_dict={\n",
    "    \"train\":TFRECORDS_TRAIN_DATA_PATH,\n",
    "    \"eval\":TFRECORDS_EVAL_DATA_PATH\n",
    "  }\n",
    "  split_names=''\n",
    "  counter=1\n",
    "\n",
    "  # Destination directory for source\n",
    "  destination_examples_artifact_uri = './examples/'\n",
    "\n",
    "  # Create a folder in destination_examples_artifact_uri for every key name\n",
    "  for key in tfrecords_dict:\n",
    "        split_name_temp = \"Split-\"+key\n",
    "        path = os.path.join(destination_examples_artifact_uri, split_name_temp)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        shutil.copy(tfrecords_dict[key], path)\n",
    "\n",
    "  # Create split_names string\n",
    "  for key in tfrecords_dict:\n",
    "      if(counter==1):\n",
    "          split_names+='[\"'+key+'\",\"'\n",
    "          counter=counter+1\n",
    "      elif(counter==2):\n",
    "          split_names+=key+'\"]'\n",
    "      else:\n",
    "          split_names+=key+','\n",
    "          counter=counter+1\n",
    "\n",
    "  print(split_names)\n",
    "\n",
    "  # CopyExampleGen execution process to replace Importer Node functionality\n",
    "  examples_importer = tfx.dsl.Importer(\n",
    "        source_uri=destination_examples_artifact_uri,\n",
    "        artifact_type=tfx.types.standard_artifacts.Examples,\n",
    "        # properties={\"split_names\": '[\"train\",\"eva\"]'}\n",
    "        properties={\"split_names\": split_names}\n",
    "        ).with_id('examples_importer')\n",
    "\n",
    "\n",
    "  hello_component = component.HelloComponent(\n",
    "      input_data=examples_importer.outputs['result'],\n",
    "      tfrecords_dict=tfrecords_dict\n",
    "    #   input_data=tfrecords_dict\n",
    "  )\n",
    "\n",
    "  # Test downstream component\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "     examples=hello_component.outputs['output_data'])\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      # example_gen,\n",
    "      examples_importer,\n",
    "      hello_component,\n",
    "      statistics_gen\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"HelloComponent\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"hello.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"metadata/copy_tfrecords/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"metadata/copy_tfrecords/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component examples_importer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"examples_importer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-03-06T22:20:37.533972\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords.examples_importer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "        additional_properties {\n",
      "          key: \"split_names\"\n",
      "          value {\n",
      "            field_value {\n",
      "              string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"./examples/\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_key\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"HelloComponent\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an importer node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Processing source uri: ./examples/, properties: {'split_names': '[\"train\",\"eval\"]'}, custom_properties: {}\n",
      "INFO:absl:Reusing existing artifact\n",
      "INFO:absl:Component examples_importer is finished.\n",
      "INFO:absl:Component HelloComponent is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"hello.component.HelloComponent\"\n",
      "  }\n",
      "  id: \"HelloComponent\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-03-06T22:20:37.533972\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords.HelloComponent\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"input_data\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"examples_importer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-03-06T22:20:37.533972\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords.examples_importer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"output_data\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"tfrecords_dict\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\\"eval\\\": \\\"../../../../Documents/tfrecord2_split_eval/data_tfrecord-00000-of-00001.gz\\\", \\\"train\\\": \\\"../../../../Documents/tfrecord1_split_train/data_tfrecord-00000-of-00001.gz\\\"}\"\n",
      "      }\n",
      "      schema {\n",
      "        value_type {\n",
      "          dict_type {\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"examples_importer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[HelloComponent] Resolved inputs: ({'input_data': [Artifact(artifact: id: 1\n",
      "type_id: 14\n",
      "uri: \"./examples/\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1677684507441\n",
      "last_update_time_since_epoch: 1677684507441\n",
      ", artifact_type: id: 14\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 179\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=179, input_dict={'input_data': [Artifact(artifact: id: 1\n",
      "type_id: 14\n",
      "uri: \"./examples/\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1677684507441\n",
      "last_update_time_since_epoch: 1677684507441\n",
      ", artifact_type: id: 14\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'output_data': [Artifact(artifact: uri: \"./artifacts/HelloComponent/output_data/179\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'tfrecords_dict': {'eval': '../../../../Documents/tfrecord2_split_eval/data_tfrecord-00000-of-00001.gz', 'train': '../../../../Documents/tfrecord1_split_train/data_tfrecord-00000-of-00001.gz'}}, execution_output_uri='./artifacts/HelloComponent/.system/executor_execution/179/executor_output.pb', stateful_working_dir='./artifacts/HelloComponent/.system/stateful_working_dir/2023-03-06T22:20:37.533972', tmp_dir='./artifacts/HelloComponent/.system/executor_execution/179/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"hello.component.HelloComponent\"\n",
      "  }\n",
      "  id: \"HelloComponent\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-03-06T22:20:37.533972\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords.HelloComponent\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"input_data\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"examples_importer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-03-06T22:20:37.533972\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords.examples_importer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"output_data\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"tfrecords_dict\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\\"eval\\\": \\\"../../../../Documents/tfrecord2_split_eval/data_tfrecord-00000-of-00001.gz\\\", \\\"train\\\": \\\"../../../../Documents/tfrecord1_split_train/data_tfrecord-00000-of-00001.gz\\\"}\"\n",
      "      }\n",
      "      schema {\n",
      "        value_type {\n",
      "          dict_type {\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"examples_importer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"copy_tfrecords\"\n",
      ", pipeline_run_id='2023-03-06T22:20:37.533972')\n",
      "INFO:absl:Value type <class 'dict'> of key tfrecords_dict in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 179 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'output_data': [Artifact(artifact: uri: \"./artifacts/HelloComponent/output_data/179\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 179\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component HelloComponent is finished.\n",
      "INFO:absl:Component StatisticsGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-03-06T22:20:37.533972\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"HelloComponent\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-03-06T22:20:37.533972\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords.HelloComponent\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"output_data\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"HelloComponent\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"train\",\"eval\"]\n",
      "From executor.py: {'tfrecords_dict': {'eval': '../../../../Documents/tfrecord2_split_eval/data_tfrecord-00000-of-00001.gz', 'train': '../../../../Documents/tfrecord1_split_train/data_tfrecord-00000-of-00001.gz'}}\n",
      "metadata_handler: <class 'tfx.orchestration.metadata.Metadata'>\n",
      "mlmd_artifact_type: <class 'ml_metadata.proto.metadata_store_pb2.ArtifactType'>\n",
      "data_type_id: <property object at 0x7f47747dfa40>\n",
      "Hello from input_artifact: Artifact(artifact: id: 1\n",
      "type_id: 14\n",
      "uri: \"./examples/\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1677684507441\n",
      "last_update_time_since_epoch: 1677684507441\n",
      ", artifact_type: id: 14\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")\n",
      "Hello from output_artifact: Artifact(artifact: uri: \"./artifacts/HelloComponent/output_data/179\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")\n",
      "Hello from output_artifact.split_names: [\"train\",\"eval\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 74\n",
      "type_id: 14\n",
      "uri: \"./artifacts/HelloComponent/output_data/179\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1678141237678\n",
      "last_update_time_since_epoch: 1678141237678\n",
      ", artifact_type: id: 14\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 180\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=180, input_dict={'examples': [Artifact(artifact: id: 74\n",
      "type_id: 14\n",
      "uri: \"./artifacts/HelloComponent/output_data/179\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\",\\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.12.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1678141237678\n",
      "last_update_time_since_epoch: 1678141237678\n",
      ", artifact_type: id: 14\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./artifacts/StatisticsGen/statistics/180\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='./artifacts/StatisticsGen/.system/executor_execution/180/executor_output.pb', stateful_working_dir='./artifacts/StatisticsGen/.system/stateful_working_dir/2023-03-06T22:20:37.533972', tmp_dir='./artifacts/StatisticsGen/.system/executor_execution/180/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2023-03-06T22:20:37.533972\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"copy_tfrecords.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"HelloComponent\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2023-03-06T22:20:37.533972\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"copy_tfrecords.HelloComponent\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"output_data\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"HelloComponent\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"copy_tfrecords\"\n",
      ", pipeline_run_id='2023-03-06T22:20:37.533972')\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:Generating statistics for split train.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Statistics for split train written to ./artifacts/StatisticsGen/statistics/180/Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to ./artifacts/StatisticsGen/statistics/180/Split-eval.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 180 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./artifacts/StatisticsGen/statistics/180\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 180\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component StatisticsGen is finished.\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      metadata_path=METADATA_PATH)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "AttributeError                            Traceback (most recent call last)\n",
      "/tmp/ipykernel_1363876/3420384479.py in <cell line: 1>()\n",
      "----> 1 tfx.orchestration.LocalDagRunner().run(\n",
      "      2   _create_pipeline(\n",
      "      3       pipeline_name=PIPELINE_NAME,\n",
      "      4       pipeline_root=PIPELINE_ROOT,\n",
      "      5       data_root=DATA_ROOT,\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/tfx_runner.py in run(self, pipeline, run_options, **kwargs)\n",
      "    122     else:\n",
      "    123       run_options_pb = None\n",
      "--> 124     return self.run_with_ir(pipeline_pb, run_options=run_options_pb, **kwargs)\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/local/local_dag_runner.py in run_with_ir(self, pipeline, run_options)\n",
      "    107           with metadata.Metadata(connection_config) as mlmd_handle:\n",
      "    108             partial_run_utils.snapshot(mlmd_handle, pipeline)\n",
      "--> 109         component_launcher.launch()\n",
      "    110         logging.info('Component %s is finished.', node_id)\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/launcher.py in launch(self)\n",
      "    571               executor_watcher.address)\n",
      "    572           executor_watcher.start()\n",
      "--> 573         executor_output = self._run_executor(execution_info)\n",
      "    574       except Exception as e:  # pylint: disable=broad-except\n",
      "    575         execution_output = (\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/launcher.py in _run_executor(self, execution_info)\n",
      "    446     outputs_utils.make_output_dirs(execution_info.output_dict)\n",
      "    447     try:\n",
      "--> 448       executor_output = self._executor_operator.run_executor(execution_info)\n",
      "    449       code = executor_output.execution_result.code\n",
      "    450       if code != 0:\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/python_executor_operator.py in run_executor(self, execution_info)\n",
      "    133         pipeline_run_id=execution_info.pipeline_run_id)\n",
      "    134     executor = self._executor_cls(context=context)\n",
      "--> 135     return run_with_executor(execution_info, executor)\n",
      "\n",
      "~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/python_executor_operator.py in run_with_executor(execution_info, executor)\n",
      "     56 \n",
      "     57   output_dict = copy.deepcopy(execution_info.output_dict)\n",
      "---> 58   result = executor.Do(execution_info.input_dict, output_dict,\n",
      "     59                        execution_info.exec_properties)\n",
      "     60   if not result:\n",
      "\n",
      "~/Repos/tfx-addons/examples/copy_tfrecords/hello/executor.py in Do(self, input_dict, output_dict, exec_properties)\n",
      "     67     # mlmd_artifact_type.properties[\"split\"] = metadata_store_pb2.STRING\n",
      "     68     # data_type_id = metadata.store.put_artifact_type(mlmd_artifact_type)\n",
      "---> 69     data_type_id = mlmd_artifact_type.store.put_artifact_type(artifact_type=types.standard_artifacts.Examples, can_add_fields=True, can_omit_fields=True)\n",
      "     70     mlmd_artifact_type.id = data_type_id\n",
      "     71     print(\"data_type_id: \" + data_type_id)\n",
      "\n",
      "AttributeError: store\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "dict_output = {\n",
    "\t\"name\": \"AttributeError\",\n",
    "\t\"message\": \"store\",\n",
    "\t\"stack\": \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\\n\\u001b[0;31mAttributeError\\u001b[0m                            Traceback (most recent call last)\\n\\u001b[0;32m/tmp/ipykernel_1363876/3420384479.py\\u001b[0m in \\u001b[0;36m<cell line: 1>\\u001b[0;34m()\\u001b[0m\\n\\u001b[0;32m----> 1\\u001b[0;31m tfx.orchestration.LocalDagRunner().run(\\n\\u001b[0m\\u001b[1;32m      2\\u001b[0m   _create_pipeline(\\n\\u001b[1;32m      3\\u001b[0m       \\u001b[0mpipeline_name\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mPIPELINE_NAME\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m       \\u001b[0mpipeline_root\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mPIPELINE_ROOT\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m       \\u001b[0mdata_root\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mDATA_ROOT\\u001b[0m\\u001b[0;34m,\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/tfx_runner.py\\u001b[0m in \\u001b[0;36mrun\\u001b[0;34m(self, pipeline, run_options, **kwargs)\\u001b[0m\\n\\u001b[1;32m    122\\u001b[0m     \\u001b[0;32melse\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    123\\u001b[0m       \\u001b[0mrun_options_pb\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;32mNone\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 124\\u001b[0;31m     \\u001b[0;32mreturn\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mrun_with_ir\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mpipeline_pb\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mrun_options\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mrun_options_pb\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m**\\u001b[0m\\u001b[0mkwargs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/local/local_dag_runner.py\\u001b[0m in \\u001b[0;36mrun_with_ir\\u001b[0;34m(self, pipeline, run_options)\\u001b[0m\\n\\u001b[1;32m    107\\u001b[0m           \\u001b[0;32mwith\\u001b[0m \\u001b[0mmetadata\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mMetadata\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconnection_config\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mmlmd_handle\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    108\\u001b[0m             \\u001b[0mpartial_run_utils\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msnapshot\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mmlmd_handle\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mpipeline\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 109\\u001b[0;31m         \\u001b[0mcomponent_launcher\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mlaunch\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    110\\u001b[0m         \\u001b[0mlogging\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m'Component %s is finished.'\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mnode_id\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/launcher.py\\u001b[0m in \\u001b[0;36mlaunch\\u001b[0;34m(self)\\u001b[0m\\n\\u001b[1;32m    571\\u001b[0m               executor_watcher.address)\\n\\u001b[1;32m    572\\u001b[0m           \\u001b[0mexecutor_watcher\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mstart\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 573\\u001b[0;31m         \\u001b[0mexecutor_output\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_run_executor\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mexecution_info\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    574\\u001b[0m       \\u001b[0;32mexcept\\u001b[0m \\u001b[0mException\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0me\\u001b[0m\\u001b[0;34m:\\u001b[0m  \\u001b[0;31m# pylint: disable=broad-except\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    575\\u001b[0m         execution_output = (\\n\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/launcher.py\\u001b[0m in \\u001b[0;36m_run_executor\\u001b[0;34m(self, execution_info)\\u001b[0m\\n\\u001b[1;32m    446\\u001b[0m     \\u001b[0moutputs_utils\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mmake_output_dirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mexecution_info\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0moutput_dict\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    447\\u001b[0m     \\u001b[0;32mtry\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 448\\u001b[0;31m       \\u001b[0mexecutor_output\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_executor_operator\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mrun_executor\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mexecution_info\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    449\\u001b[0m       \\u001b[0mcode\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mexecutor_output\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mexecution_result\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mcode\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    450\\u001b[0m       \\u001b[0;32mif\\u001b[0m \\u001b[0mcode\\u001b[0m \\u001b[0;34m!=\\u001b[0m \\u001b[0;36m0\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/python_executor_operator.py\\u001b[0m in \\u001b[0;36mrun_executor\\u001b[0;34m(self, execution_info)\\u001b[0m\\n\\u001b[1;32m    133\\u001b[0m         pipeline_run_id=execution_info.pipeline_run_id)\\n\\u001b[1;32m    134\\u001b[0m     \\u001b[0mexecutor\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0m_executor_cls\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mcontext\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mcontext\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 135\\u001b[0;31m     \\u001b[0;32mreturn\\u001b[0m \\u001b[0mrun_with_executor\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mexecution_info\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mexecutor\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\n\\u001b[0;32m~/.pyenv/versions/3.8.2/envs/tfx-3.8.2/lib/python3.8/site-packages/tfx/orchestration/portable/python_executor_operator.py\\u001b[0m in \\u001b[0;36mrun_with_executor\\u001b[0;34m(execution_info, executor)\\u001b[0m\\n\\u001b[1;32m     56\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     57\\u001b[0m   \\u001b[0moutput_dict\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mcopy\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mdeepcopy\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mexecution_info\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0moutput_dict\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m---> 58\\u001b[0;31m   result = executor.Do(execution_info.input_dict, output_dict,\\n\\u001b[0m\\u001b[1;32m     59\\u001b[0m                        execution_info.exec_properties)\\n\\u001b[1;32m     60\\u001b[0m   \\u001b[0;32mif\\u001b[0m \\u001b[0;32mnot\\u001b[0m \\u001b[0mresult\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\n\\u001b[0;32m~/Repos/tfx-addons/examples/copy_tfrecords/hello/executor.py\\u001b[0m in \\u001b[0;36mDo\\u001b[0;34m(self, input_dict, output_dict, exec_properties)\\u001b[0m\\n\\u001b[1;32m     67\\u001b[0m     \\u001b[0;31m# mlmd_artifact_type.properties[\\\"split\\\"] = metadata_store_pb2.STRING\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     68\\u001b[0m     \\u001b[0;31m# data_type_id = metadata.store.put_artifact_type(mlmd_artifact_type)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m---> 69\\u001b[0;31m     \\u001b[0mdata_type_id\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mmlmd_artifact_type\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mstore\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mput_artifact_type\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0martifact_type\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mtypes\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mstandard_artifacts\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mExamples\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mcan_add_fields\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mcan_omit_fields\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0;32mTrue\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m     70\\u001b[0m     \\u001b[0mmlmd_artifact_type\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mid\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mdata_type_id\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m     71\\u001b[0m     \\u001b[0mprint\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m\\\"data_type_id: \\\"\\u001b[0m \\u001b[0;34m+\\u001b[0m \\u001b[0mdata_type_id\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\n\\u001b[0;31mAttributeError\\u001b[0m: store\"\n",
    "}\n",
    "ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "result = ansi_escape.sub('', dict_output[\"stack\"])\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx-3.8.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a21382bcf4cd2ec5bec501ec961be81f5e055eb8968eab00b0db6b308c7e2937"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
